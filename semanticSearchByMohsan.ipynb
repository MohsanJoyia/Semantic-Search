{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------- \n",
      "                                                 text category\n",
      "0      Meditation and yoga can improve mental health   Health\n",
      "1  Fruits, whole grains and vegetables helps cont...   Health\n",
      "2  These are the latest fashion trends for this week  Fashion\n",
      "3  Vibrant color jeans for male are becoming a trend  Fashion\n",
      "4                 The concert starts at 7 PM tonight    Event\n",
      "5  Navaratri dandiya program at Expo center in Mu...    Event\n",
      "6  Exciting vacation destinations for your next trip   Travel\n",
      "7  Maldives and Srilanka are gaining popularity i...   Travel\n",
      "---------------------------------------------------------------- \n",
      " (8, 768)\n",
      "768\n",
      "---------------------------------------------------------------- \n",
      " <faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x000002418089D110> >\n",
      "---------------------------------------------------------------- \n",
      " (768,)\n",
      "---------------------------------------------------------------- \n",
      " (1, 768)\n",
      "---------------------------------------------------------------- \n",
      " [[3 2]]\n",
      "---------------------------------------------------------------- \n",
      "                                                 text category\n",
      "3  Vibrant color jeans for male are becoming a trend  Fashion\n",
      "2  These are the latest fashion trends for this week  Fashion\n",
      "---------------------------------------------------------------- \n",
      " I want to  buy a polo t-shirt\n"
     ]
    }
   ],
   "source": [
    "# here we use the pandas library to load our csv file which has some kind of text\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('D:/New folder/sample_text.csv')\n",
    "print(\"---------------------------------------------------------------- \\n\", dataset)\n",
    "\n",
    "# here we first convert the main data text into vectors using sentencet_trasformeers.\n",
    "# in vectors basically we give \"importance weight\" to each word \n",
    "# and use \"all-mpnet-base-v2\" this model from hugging face which convrt our \"text\" name column in the file into vectors.\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "encoder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "vector = encoder.encode(dataset['text'])\n",
    "print(\"---------------------------------------------------------------- \\n\",vector.shape)\n",
    "\n",
    "# now from above we use only the main 'text' column  vector which is on index 1.\n",
    "# FAISS (Facebook AI faiss Search) is a library developed by Facebook AI Research that \n",
    "# is designed for efficient faiss search and clustering of dense vectors. \n",
    "# The main concept behind FAISS is to quickly find the nearest neighbors of a given query vector in a large dataset of vectors.\n",
    "dem= vector.shape[1] \n",
    "print(dem)\n",
    "import faiss\n",
    "faiss_weightsVector_main_text= faiss.IndexFlatL2(dem)\n",
    "print(\"---------------------------------------------------------------- \\n\",faiss_weightsVector_main_text)\n",
    "\n",
    "# now we add main vector which has all indexes in faiss.\n",
    "faiss_weightsVector_main_text.add(vector)\n",
    "\n",
    "\n",
    "# here we muse some search query to check our model.\n",
    "# and again give importance weight to search query and make vector of search query.\n",
    "import numpy as np\n",
    "search_query='I want to  buy a polo t-shirt'\n",
    "vec = encoder.encode(search_query)\n",
    "print(\"---------------------------------------------------------------- \\n\",vec.shape)\n",
    "\n",
    "\n",
    "#  as search query vector is only one dimentional so we convert it into two dimentional by reshaping.\n",
    "svec=np.array(vec).reshape(1,-1)\n",
    "print(\"---------------------------------------------------------------- \\n\",svec.shape)\n",
    "\n",
    "\n",
    "# here we search the our query in main text data\n",
    "distances , I= faiss_weightsVector_main_text.search(svec, k=2)\n",
    "\n",
    "# it will give index of findings\n",
    "print(\"---------------------------------------------------------------- \\n\",I)\n",
    "\n",
    "# \n",
    "print(\"---------------------------------------------------------------- \\n\",dataset.loc[[3,2]])\n",
    "\n",
    "# \n",
    "\n",
    "print(\"---------------------------------------------------------------- \\n\",search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
